{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yang harus di instal\n",
    "!pip install google-api-python-client\n",
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2\n",
    "!pip install pickle\n",
    "!pip install sastrawi\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#library yang digunakan, jika dirasa kurang penting dapat dihapus\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bab4d730a0d10f78"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2042a6c8ea26436"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Call the \"build()\" function from the Python-client\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = input(\"API KEY: \")\n",
    "youtube = build(\"youtube\",\"v3\", developerKey=api_key)\n",
    "url = input(\"VIDEOURL: \")\n",
    "\n",
    "def get_comments(url):\n",
    "    # Get the ID of the video by splitting the URL\n",
    "    single_video_id = url.split(\"=\")[1].split(\"&\")[0]\n",
    "    # Use the list() method to extract a JSON with key information\n",
    "    # from the video.\n",
    "    video_list=youtube.videos().list(part=\"snippet\",id=single_video_id).execute()\n",
    "    channel_id= video_list[\"items\"][0][\"snippet\"][\"channelId\"]\n",
    "    title_single_video= video_list[\"items\"][0][\"snippet\"][\"title\"]\n",
    "    playlist_id = None\n",
    "    forUserName = None\n",
    "\n",
    "    nextPageToken_comments = None\n",
    "    commentsone=[]\n",
    "\n",
    "    while True:\n",
    "        #Request the first 50 videos of a channel. This is the full dictionary. The result is store in a variable called \"pl_response\".\n",
    "        #PageToken at this point is \"None\"\n",
    "        pl_request_comment= youtube.commentThreads().list(part=[\"snippet\",\"replies\"],\n",
    "                                            videoId=single_video_id,\n",
    "                                            maxResults=50,\n",
    "                                            pageToken= nextPageToken_comments)\n",
    "        pl_response_comment = pl_request_comment.execute()\n",
    "\n",
    "        ## Send the amount of views and the URL of each video to the videos empty list that was declared at the beginning of the code.\n",
    "        for i in pl_response_comment[\"items\"]:\n",
    "            vid_comments = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n",
    "            comm_author = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "            comm_author_id = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"]\n",
    "            comm_date = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            comm_likes = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "            new_var=i.get(\"replies\",\"0\")\n",
    "\n",
    "            commentsone.append({\n",
    "                \"comm_date\":comm_date,\n",
    "                \"author\":comm_author,\n",
    "                \"author_id\":comm_author_id,\n",
    "                \"likes\":comm_likes,\n",
    "                \"comment\":vid_comments,\n",
    "                \"video_id\":single_video_id\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "        nextPageToken_comments = pl_response_comment.get(\"nextPageToken\")\n",
    "\n",
    "        if not nextPageToken_comments:\n",
    "            break\n",
    "\n",
    "    for i in commentsone[:10]:\n",
    "        print(i[\"comment\"])\n",
    "\n",
    "\n",
    "    pd.DataFrame.from_dict(commentsone).to_csv(f\"/content/drive/MyDrive/comments/dataset.csv\")\n",
    "\n",
    "get_comments(url)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7e0f429893509c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/comments/dataset.csv')\n",
    "df.head(500)\n",
    "df.count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6083d65cd112f718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/comments/dataset.csv\")\n",
    "data = data.dropna()\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e1712e0bde0ab92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_nw = data.drop(['comm_date',\"author\", 'author_id',\"likes\",'video_id'], axis=1 )\n",
    "data_nw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11c5630032ced73f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_nw.to_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\") #Fungsinya untuk menyimpan hasil drop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29dc63982074be3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_baru = pd.read_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\")\n",
    "data_baru.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccea44824b6c53a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def caseFolding(comment):\n",
    "          comment = comment.lower()\n",
    "          comment = comment.strip(\" \")\n",
    "          comment = re.sub(r'[?|$|.|!]',r'', comment)\n",
    "          comment = re.sub(r'[^a-zA-Z0-9 ]',r'', comment)\n",
    "          return comment\n",
    "\n",
    "data_baru['comment'] = data_baru['comment'].apply(caseFolding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f8f1ea03b28cb32"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_baru.to_csv(\"/content/drive/MyDrive/comments/dataset_bersih.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4c18ad2bdd5d554"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/content/drive/MyDrive/comments/dataset_bersih.csv')\n",
    "X = data['comment']\n",
    "y = data['sentimen']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77d8a5138ac4d584"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan preprocessing pada teks\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "X = X.apply(preprocessing)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826036f941edda5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan vectorization pada teks\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "832fe427cc356fc3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan vectorization pada teks\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bff1a70ce115f67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan pembagian dataset menjadi data training dan data testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba7afc2d898f06ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan training model KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca1dda07242ee748"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan prediksi pada data testing\n",
    "y_pred = knn.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae6831e4eb99b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Lakukan evaluasi model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)\n",
    "print('Confusion Matrix:\\n', cm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f17e2190c16c95b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

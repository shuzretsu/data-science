{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 1: Instalasi Library</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "770203f8e333a06f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2\n",
    "!pip install sastrawi textblob wordcloud nltk scikit-learn\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53f45872f15cb4a3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 2: Import Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2328661691e6f7f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e9d43fe754832d1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 3: Mengambil Komentar dari YouTube"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7db53d023f331a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = input(\"API KEY: \")\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "url = input(\"VIDEO URL: \")\n",
    "\n",
    "def get_comments(url):\n",
    "    single_video_id = url.split(\"=\")[1].split(\"&\")[0]\n",
    "    video_list = youtube.videos().list(part=\"snippet\", id=single_video_id).execute()\n",
    "    channel_id = video_list[\"items\"][0][\"snippet\"][\"channelId\"]\n",
    "    title_single_video = video_list[\"items\"][0][\"snippet\"][\"title\"]\n",
    "\n",
    "    nextPageToken_comments = None\n",
    "    commentsone = []\n",
    "\n",
    "    while True:\n",
    "        pl_request_comment = youtube.commentThreads().list(\n",
    "            part=[\"snippet\", \"replies\"],\n",
    "            videoId=single_video_id,\n",
    "            maxResults=50,\n",
    "            pageToken=nextPageToken_comments\n",
    "        )\n",
    "        pl_response_comment = pl_request_comment.execute()\n",
    "\n",
    "        for i in pl_response_comment[\"items\"]:\n",
    "            vid_comments = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n",
    "            comm_author = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "            comm_author_id = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"]\n",
    "            comm_date = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            comm_likes = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "\n",
    "            commentsone.append({\n",
    "                \"comm_date\": comm_date,\n",
    "                \"author\": comm_author,\n",
    "                \"author_id\": comm_author_id,\n",
    "                \"likes\": comm_likes,\n",
    "                \"comment\": vid_comments,\n",
    "                \"video_id\": single_video_id\n",
    "            })\n",
    "\n",
    "        nextPageToken_comments = pl_response_comment.get(\"nextPageToken\")\n",
    "        if not nextPageToken_comments:\n",
    "            break\n",
    "\n",
    "    for i in commentsone[:10]:\n",
    "        print(i[\"comment\"])\n",
    "\n",
    "    pd.DataFrame.from_dict(commentsone).to_csv(\"dataset.csv\")\n",
    "\n",
    "get_comments(url)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961dc0fe25267997",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 4: Menampilkan Hasil Scraping\n",
    "</h4>\n",
    "\n",
    "Menampilkan hasil scraping dari file dataset.csv."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26d98b28dfdef95b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.head(500)\n",
    "df.count()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf13e0aee3a89dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 5: Membersihkan Data\n",
    "</h4>\n",
    "\n",
    "Menghapus kolom yang tidak diperlukan."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8730aa93335c5b2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "data = data.dropna()\n",
    "print(data.head())\n",
    "\n",
    "data_nw = data.drop(['comm_date', \"author\", 'author_id', \"likes\", 'video_id'], axis=1)\n",
    "data_nw.to_csv(\"dataset_drop.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b80c32cf2729be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 6: Membuka Dataset yang Telah Dibersihkan"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e8bcbfa70f8afd0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_baru = pd.read_csv(\"dataset_drop.csv\")\n",
    "data_baru.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "460c1523e611aaca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 7: Preprocessing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cea4a771741a046"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def caseFolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!]', r'', comment)\n",
    "    comment = re.sub(r'[^a-zA-Z0-9 ]', r'', comment)\n",
    "    return comment\n",
    "\n",
    "data_baru['comment'] = data_baru['comment'].apply(caseFolding)\n",
    "data_baru.to_csv(\"dataset_bersih.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8220490347b75f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 8: Memberi Label Sentimen<br>\n",
    "\n",
    "Secara manual, tambahkan kolom 'sentimen' pada dataset_bersih.csv dengan nilai 'positif', 'negatif', atau 'netral'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0dc7e6c34c207b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9e58d6d7007a34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Langkah 9: Klasifikasi Sentimen Menggunakan KNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e4e3d893e14abe3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data = pd.read_csv('dataset_bersih.csv')\n",
    "X = data['comment']\n",
    "y = data['sentimen']\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "X = X.apply(preprocessing)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc19309e3032d8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dedb24512b9bbf1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b77db1491bf44108"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6a12db945a6701f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "y_pred = knn.predict(X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b78254a2282fe3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdb2bfb9f8683b43"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99e742fc29719d04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "591248b08976b18e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be926bd9753ba60e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7570dc8173af1db7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
